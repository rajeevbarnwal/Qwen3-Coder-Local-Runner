{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§  Qwen3-Coder Local Runner (Colab Setup)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!apt-get update && apt-get install -y build-essential cmake curl git pciutils libcurl4-openssl-dev"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install huggingface_hub hf_transfer transformers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp && cd llama.cpp && \\\n",
    "cmake . -B build -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=OFF -DLLAMA_CURL=ON && \\\n",
    "cmake --build build --config Release -j"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "snapshot_download(\n",
    "    repo_id='unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF',\n",
    "    local_dir='Qwen3-Coder',\n",
    "    allow_patterns=['*UD-Q2_K_XL*']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘‰ Now switch to the `llama.cpp` folder and run `llama-cli` with your downloaded GGUF model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"name": "python3","display_name": "Python 3"},
  "language_info": {"name": "python"}
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

